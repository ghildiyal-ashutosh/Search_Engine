Snippets for the query:  Q50
************************
Total Hits:  58
************************

************************
Document:  CACM-2664.html
************************


parallelism in tape-sorting

two methods for employing parallelism in tape-sorting
are presented.  method a is the natural 
way to use parallelism. method b is new.  both approximately
achieve the goal of reducing the processing 
time by a divisor which is the number of processors.

cacm april, 1974


************************

************************
Document:  CACM-3097.html
************************
************************
Document:  CACM-2645.html
************************


two languages for estimating program efficiency

two languages enabling their users to estimate
the efficiency of computer programs are presented. 
 the program whose efficiency one wishes to estimate is written
in the first language, a go-to-less programming 
language which includes most of the features of algol
60.  the second language consists of interactive 
commands enabling its users to provide additional information
about the program written in the first 
language and to output results estimating its efficiency.
 processors for the two languages are also 
described.  the first processor is a syntax-directed
translator which compiles a program into a symbolic 
formula representing the execution time for that program.
 the sound processor is a set of procedures 
for that program.  the second processor is a set of
procedures for algebraic manipulation which can be 
called by the user to operate on the formula produced
by the first processor.  examples of the usage 
of the two languages are included.  the limitations of
the present system, its relation to knuth's work 
on the analysis of algorithms, and some of the directions

************************

************************
Document:  CACM-2884.html
************************
************************
Document:  CACM-2692.html
************************


reentrant polygon clipping

a new family of clipping algorithms is described.
 these algorithms are able to clip polygons 
against irregular convex plane-faced volumes in three
dimensions, removing the parts of the polygon which 
lie outside the volume.  in two dimensions the algorithms
permit clipping against irregular convex windows. 
 polygons to be clipped are represented as an ordered
sequence of vertices without repetition of first 
and last, in marked contrast to representation as a
collection of edges as was heretofore the common 
procedure.  output polygons have an identical format,
with new vertices introduced in sequence to describe 
any newly-cut edge or edges.  the algorithms easily handle
the particularly difficult problem of detecting 
that a new vertex may be required at a corner of the
clipping window.  the algorithms described achieve 
considerable simplicity by clipping separately against
each clipping plane or window boundary.  code 
capable of clipping the polygon against a single boundary
is reentered to clip against subsequent boundaries. 
 each such reentrant stage of clipping need store only
two vertex values and may begin its processing 
as soon as the first output vertex from the proceeding
stage is ready.  because the same code is reentered 
for clipping against subsequent boundaries, clipping
against very complex window shapes is practical. 
 for perspective applications in three dimentions, a six-plane
truncated pyramid is chosen as the clipping 
volume.  the two additional planes parallel to the projection
screen 
************************

************************
Document:  CACM-3059.html
************************
************************
Document:  CACM-1795.html
************************
************************
Document:  CACM-1665.html
************************
************************
Document:  CACM-3152.html
************************


storage reorganization techniques for
matrix computation in a paging environment

in order to multiply matrices while minimizing
the number of page fetches required, it is often more efficient to
reorganize the data into submatrix form and to use block multiplication 
rather than to use the best known algorithms which leave the
matrices 
************************

************************
Document:  CACM-2068.html
************************
************************
Document:  CACM-2863.html
************************


vmin-an optimal variable-space page replacement algorithm

a criterion for comparing variable space page
replacement algorithms is presented.  an optimum 
page replacement algorithm, called vmin, is described and
shown to be optimum with respect to this criterion. 
the results of simulating vmin, denning's working set,
and the page partitioning replacement algorithms 
on five virtual memory programs are presented to demonstrate
the improvement possible over the known 
realizable variable space algorithms.

cacm may, 1976

prieve, b. g.
fabry, r. s.

demand paging, performance measurement, 
************************

************************
Document:  CACM-2283.html
************************
************************
Document:  CACM-2872.html
************************
************************
Document:  CACM-0082.html
************************
************************
Document:  CACM-3156.html
************************


computing connected components on parallel computers

we present a parallel algorithm which uses n2 processors to find the connected

************************
 to find the connected
components of an undirected graph with n vertices in time o(log2n).  an
o(log2n) time bound also can be achieved using only n$n/$log2n)) processors.
the algorithm can be used to find the transitive closure
of a symmetric boolean matrix.  we assume that the processors have
access to a common memory.  simultaneous access to the same location
is permitted for fetch instructions but not for store instructions.

cacm august, 1979

hirschberg, d.
chandra, a.
sarwate, d.

graph theory, parallel processing, algorithms,
transitive closure, 
************************

************************
Document:  CACM-2266.html
************************


a highly parallel algorithm for approximating
all zeros of a polynomial with only real zeros

an algorithm is described based on newton's
method which simultaneously approximates all zeros 
of a polynomial with only real zeros.  the algorithm, which
is conceptually suitable for parallel computation, 
determines its own starting values so that convergence
to the zeros is guaranteed.  multiple zeros and 
their multiplicity are readily determined.  at no
point in the method is polynomial deflation used.

cacm november, 1972

patrick, m. l.

parallel numerical algorithms, real polynomials,
real 
************************

************************
Document:  CACM-2137.html
************************
************************
Document:  CACM-2896.html
************************
************************
Document:  CACM-1964.html
************************
************************
Document:  CACM-2342.html
************************
************************
Document:  CACM-2522.html
************************


the design, implementation, and evaluation of a working set dispatcher

the behavior of a computer system is largely
dependent upon the algorithms employed to allocate 
the system resources to the processes competing for them.
 recent research in time-sharing paging systems 
has developed 
************************

************************
Document:  CACM-3006.html
************************


anomalies with variable partition paging algorithms

five types of anomalous 
************************


five types of anomalous behavior which may
occur in paged virtual memory operating systems 
a redefined.  one type of anomaly, for example, concerns
the fact that, with certain reference strings 
and paging algorithms, an increase in mean 
************************
, an increase in mean memory allocation
may result in an increase in fault rate. 
 two paging algorithms, are examined in terms 
************************
, are examined in terms of their
anomaly potential, and reference string examples 
of various anomalies are presented.  two paging algorithm
properties, the inclusion property and the 
generalized inclusion property, are discussed and the
anomaly implications of these properties presented.

cacm march, 1978

franklin, m.
graham, g.
gupta, r.

anomaly, memory management, program behavior, stack
algorithms, virtual memory, working 
************************
, virtual memory, working set, page 
fault frequency, paging algorithms

4.32 4.35 4.6 8.1


************************

************************
Document:  CACM-1302.html
************************
************************
Document:  CACM-2365.html
************************


matrix computations with fortran and paging

the efficiency of conventional fortran programs
for matrix computations can often be improved 
by reversing the order of nested loops.  such modifications
produce modest savings in many common situations 
and very significant savings for large problems run
under an operating system which uses paging.

cacm april, 1972

moler, c. b.

matrix algorithms, linear equations, fortran,

************************

************************
Document:  CACM-0371.html
************************
************************
Document:  CACM-2838.html
************************


analysis of an algorithm for real time garbage collection

a real time garbage collection system avoids
suspending the operations of a list processor 
for the long times that garbage collection normally requires
by performing garbage collection on a second 
processor in parallel with list processing operations,
or on a single processor time-shared with them. 
 algorithms for recovering discarded 
************************
 for recovering discarded list structures in
this manner are presented and analyzed to determine 
sufficient conditions under which the list processor never
needs to wait on the collector.  these techniques 
are shown to require at most twice as much processing
power as regular garbage collectors, if they are 
used efficiently.  the average behavior of the program
is shown to be very nearly equal to the worst-case 
performance, so that the sufficient conditions are also
suitable for measuring the typical behavior of 
the algorithm.

cacm september, 1976

wadler, p. l.

garbage collection, storage reclamation, list
processing, lisp, time-sharing, multiprocessing, 
parallel processing, real time, data structures, analysis of algorithms

3.69 3.89 4.19 4.29 
************************

************************
Document:  CACM-2627.html
************************
************************
Document:  CACM-2557.html
************************


on the time required for a sequence of matrix products

this paper discusses the multiplication of conformable
sequences of row vectors, column vectors, 
and square matrices.  the minimum time required to evaluate
such products on ordinary serial computers 
as well as parallel computers is discussed.  algorithms
are presented which 
************************

************************
Document:  CACM-3088.html
************************
************************
Document:  CACM-2700.html
************************
************************
Document:  CACM-1924.html
************************


organizing matrices and matrix operations for paged memory systems

matrix representations and operations are examined
for the purpose of minimizing the page faulting 
occurring in a paged memory system.  it is shown that
carefully designed matrix algorithms can lead to 
enormous savings in the number of page faults occurring
when only a small part of the total matrix can 
be in main memory at one time.  examination of addition,
multiplication, and inversion algorithms shows 
that a partitioned matrix representation (i.e. one submatrix
or partition per page) in most cases induced 
fewer page faults than a row-by-row representation.
 the number of page-pulls required by these matrix 
manipulation algorithms is also studied as a function
of the number of pages of main memory available 
to the algorithm.

cacm march, 1969

mckellar, a. c.
coffman jr., e. g.

matrix algorithms, array processing, paging algorithms,
paged memory 
************************

************************
Document:  CACM-1468.html
************************


syntax-directed interpretation of classes of pictures

a descriptive scheme for classes of pictures based
on labeling techniques using parallel processing 
algorithms was proposed by the 
************************

************************
Document:  CACM-3075.html
************************


fast parallel sorting algorithms

a parallel bucket-sort 
************************


a parallel bucket-sort algorithm is presented
that requires time o(log n) and the use of n 
processors.  the algorithm makes 
************************
.  the algorithm makes use of a technique that
requires more space than the product of processors 
and time.  a realistic model is used model is used in which
no memory contention is permitted.  a procedure 
is also presented to sort n numbers in time o(k log
n) using n 1 + 1/k processors, for k an arbitrary 
integer.  the model of computation for this procedure
permits simultaneous fetches from the same memory 
location.

cacm august, 1978

hirschberg, d.

parallel processing, sorting, algorithms, bucket sort

3.74 4.34 
************************

************************
Document:  CACM-2175.html
************************


subexpression ordering in the execution of arithmetic expressions

an arithmetic expression can often be broken
down into its component subexpressions.  depending 
on the hardware environment in which the expression is
to be executed, these subexpressions can be evaluated 
in serials, in parallel, or in a combination of these
modes.  this paper shows that expression execution 
time can be minimized only if consideration is given to
the ordering of the subexpressions.  in particular, 
subexpressions should be executed in order of decreasing
memory and processor time requirements.  this 
observation is valid for configurations ranging from
a uniprocessor with an unbuffered main memory to 
multiprocessor with a "cache" buffer memory.  if the
number of subexpressions which can be executed in 
parallel exceeds the number of available processors,
then execution of some 
************************

************************
Document:  CACM-2017.html
************************
************************
Document:  CACM-2433.html
************************


control structures in illiac iv fortran

as part of an effort to design and implement
a fortran compiler on the illiac iv, an extended 
fortran, called ivtran, has been developed.  this language
provides a means of expressing data and control 
structures suitable for exploiting illiac iv parallelism.
 this paper reviews the hardware characteristics 
of the illiac and singles out unconventional features
which could be expected to influence language (and 
compiler) design.  the implications of these features for
data layout and algorithm structure are discussed, 
and the conclusion is drawn that data allocation rather than
code structuring is the crucial illiac optimization 
problem.  a satisfactory method of data allocation is
then presented.  language structures to utilize 
this storage method and express parallel algorithms are described.

cacm 
************************

************************
Document:  CACM-1367.html
************************
************************
Document:  CACM-2226.html
************************
************************
Document:  CACM-1825.html
************************
************************
Document:  CACM-1811.html
************************


a case study in programming for parallel-processors

an affirmative partial 
************************


an affirmative partial answer is provided to
the question of whether it is possible to program 
parallel-processor computing systems to efficiently decrease
execution time for useful problems.  parallel-processor 
systems are multiprocessor systems in which several of
the processors can simultaneously execute 
************************
 can simultaneously execute separate 
tasks of a single job, thus cooperating to decrease
the solution time of a computational problem. the 
processors have independent instruction counters, meaning
that each processor executes its own task program 
relatively independently of the other processors.  communication
between cooperating processors is by 
means of data in storage shared by all processors.  a
program for the determination of the distribution 
of current in an electrical network was written for a
parallel-processor computing 
************************
-processor computing system, and execution 
of this program was simulated.  the data gathered from
simulation runs demonstrate the efficient solution 
of this problem, typical of a large class of important
problems.  it is shown that, with proper programming, 
solution time when n processors are applied approaches
1/n times the solution time for a single processor, 
while improper programming can actually lead to an increase
of solution time with the number of processors. 
 stability of the method of solution was also investigated.

cacm december, 1969

rosenfeld, j. l.

parallel-processor, parallelism, 
************************

************************
Document:  CACM-1658.html
************************
************************
Document:  CACM-0950.html
************************


parallel methods for integrating ordinary differential equations

this paper is dedicated to the proposition that,
in order to take full advantage for real-time 
computations of highly parallel computers as can be
expected to be available in the near future, much 
of numerical analysis will have to be recast in a more
"parallel" form.  by this is meant that serial 
algorithms ought to be replaced 
************************

************************
Document:  CACM-2895.html
************************
************************
Document:  CACM-2685.html
************************
************************
Document:  CACM-2085.html
************************
************************
Document:  CACM-2914.html
************************
************************
Document:  CACM-2669.html
************************
************************
Document:  CACM-0392.html
************************
************************
Document:  CACM-2166.html
************************
************************
Document:  CACM-2973.html
************************


sorting on a mesh-connected parallel computer

two algorithms are presented for sorting 
************************
 are presented for sorting n^2
elements on an n x n mesh-connected processor 
array that require o(n) routing and comparison steps.
 the best previous algorithm takes time o(n log 
n).  the algorithms of this paper are shown to be optimal
in time within small constant factors.  extensions 
to higher-dimensional arrays are also given.

cacm april, 1977

thompson, c. d.
kung, h. t.

parallel computer, parallel sorting, 
************************

************************
Document:  CACM-2950.html
************************
************************
Document:  CACM-2865.html
************************
************************
Document:  CACM-2723.html
************************


multiprocessing compactifying garbage collection

algorithms for a multiprocessing compactifying
garbage collector are presented and discussed. 
 the simple case of two processors, one performing lisp-like

************************
, one performing lisp-like
list operations and the other performing 
garbage collection continuously, is thoroughly examined.
the necessary capabilities of each processor 
are defined, as well as interprocessor communication
and interlocks. complete procedures for garbage 
collection and for standard list processing primitives
are presented and thoroughly explained.  particular 
attention is given to the problems of marking and relocating
list cells while another processor may be 
operating on them.  the primary aim throughout is to
allow the list processor to run unimpeded while 
the other processor reclaims list storage.  the more
complex cases involving several list processors 
and one or more garbage collection processors are also briefly discussed.

cacm september, 1975

steele, g. l. jr.

garbage collection, storage reclamation, reclaimer,
storage allocation, multiprocessing, synchronization, 
semaphores, parallel processing, compactification, 
************************

************************
Document:  CACM-1753.html
************************
************************
Document:  CACM-0303.html
************************
************************
Document:  CACM-2903.html
************************
************************
Document:  CACM-2570.html
************************


a comparison of list schedules for parallel processing systems

the problem of scheduling two or more processors
to minimize the execution 
************************

to minimize the execution time of a program 
which consists of a set of partially ordered tasks
is studied.  cases where task execution times are 
deterministic and others in which execution times are
random variables are analyzed.  it is shown that 
different algorithms suggested in the literature vary significantly
in execution time and that the b-schedule 
of coffman and graham is near-optimal.  a dynamic programming
solution for the case in which execution 
times are random variables is presented.

cacm december, 1974

adam, t. l.
chandy, k. m.
dickson, j. r.

parallel processing, precedence 
************************

************************
Document:  CACM-2195.html
************************
************************
Document:  CACM-2022.html
************************
************************
Document:  CACM-1601.html
************************


parallel numerical methods for the solution of equations

classical iterative procedures for the numerical
solution of equations provide at each stage 
a single new approximation to the root in question.  a
technique is given for the development of numerical 
procedures which provide, at each stage, several approximations
to a solution of an equation.  the s8everal 
approximations obtained in any iteration are computationally
independent, making the methods of interest 
in a parallel processing environment.  convergence is
insured by extracting the "best information" at 
each iteration.  several families of numerical procedures
which use the technique of the procedures in 
a parallel processing environment are developed and measurements
of these statistics are reported.  these 
measurements are interpreted in a parallel processing
environment.  in such an environment the procedures 
obtained are superior to standard algorithms.

cacm may, 1967

shedler, 
************************

************************
Document:  CACM-2297.html
************************
************************
Document:  CACM-2526.html
************************
************************
Document:  CACM-1846.html
************************
************************
Document:  CACM-1810.html
************************


is automatic "folding" of programs efficient enough to displace manual?

the operation of "folding" a program into
the available memory is discussed.  measurements 
by brown et al. and by nelson on an automatic folding
mechanism of simple design, a demand paging unit 
built at the ibm research center by belady, nelson,
o'neil, and others, permitting its quality to be 
compared with that of manual folding, are discussed,
and it is shown that given some care in use the 
unit performs satisfactorily under the conditions tested,
even though it is operating across a memory-to-storage 
interface with a very large speed difference.  the disadvantages
of prefolding, which is required when 
the folding is manual, are examined, and a number of
the important troubles which beset computing today 
are shown to arise from, or be aggravated by, this
source.  it is concluded that a folding mechanism 
will probably become a normal part of most computing systems.

cacm december, 1969

sayre, d.

paging, automatic paging, demand paging, folding,
automatic folding, storage hierarchies, memory 
hierarchies, replacement algorithms, performance, measurement

************************

************************
Document:  CACM-1988.html
************************
************************
Document:  CACM-2450.html
************************


empirical working set behavior

the working set model for program behavior
has been proposed in recent years as a basis for 
the design of scheduling and paging algorithms.  although
the words 
************************

************************
Document:  CACM-2851.html
************************
************************
Document:  CACM-2881.html
************************
************************
Document:  CACM-2114.html
************************


a formal system for information retrieval from files

a generalized file structure is provided
by which the concepts of keyword, index, record, file, directory,
file structure, directory decoding, and record retrieval are defined
and from which some of the frequently used file structures such
as inverted files, index-sequential files, and multilist files are
derived.  two algorithms which retrieve records from the generalized file 
structure are presented.

cacm february, 1970

hsiao, d.

attribute-value pair, index, keyword, record, record address,
k-pointer, k-list, file, directory, generalized file
structure, inverted file, index-sequential-file, multilist file,
description, file search, directory search, serial processing of
lists, prime keyword, parallel processing of lists 
************************

************************
Document:  CACM-2497.html
************************


synchronizing processors with memory-content-generated interrupts

implementations of the "lock-unlock" method
of synchronizing processors in a multiprocessor 
system usually require uninterruptable, memory-pause type instructions.
 an interlock scheme called read-interlock, 
which does not require memory-pause instructions, has
been developed for a dual dec pdp-10 system with 
real-time requirements.  the read-interlock method does
require a special"read-interlock" instruction 
in the repertoire of the processors and a special "read-interlock"
cycle in the repertoire of the memory 
modules.  when a processor examines a "lock" (a memory
location) with a read-interlock instruction, it 
will be interrupted if the lock was already set; examining
a lock immediately sets it if it was not already 
set (this event sequence is a read-interlock cycle). 
writing into a lock clears it.  having the processor 
interrupted upon encountering a set lock instead of
branching is advantageous if the branch would have 
resulted in an effective interrupt.

cacm june, 1973

hill, j. c.

interrupts,supervisors, monitors, debugging, parallel
processing, associative 
************************

************************
Document:  CACM-2373.html
************************


properties of the working-set model

a program's working set w(t,t) at time t is
the set of distinct pages among the t most recently 
referenced pages.  relations between the average working-set size,
the missing-page rate, and the interreference-interval 
distribution may be derived both from time-average definitions
and from ensemble-average (statistical) 
definitions. an efficient algorithm for estimating these
quantities is given.  the relation to lru (least 
recently used) paging is characterized.  the independent-reference
model, in which page references are 
statistically independent, is used to assess the effects
to interpage dependencies on working-set size 
observations. under general assumptions, working-set
size is shown to be normally distributed.

cacm march, 1972

denning, p. j.
schwartz, s. c.

working-set model, paging, paging algorithms,
program behavior, program 
************************

************************
Document:  CACM-2262.html
************************
************************
Document:  CACM-1923.html
************************
************************
Document:  CACM-2498.html
************************


minimizing wasted space in partitioned segmentation

a paged virtual memory system using a finite
number of page sizes is considered.  two algorithms 
for assigning pages to segments are discussed.  both
of these algorithm are simple to implement.  the 
problem of choosing the page sizes to minimize the expected
value of total wasted space in internal fragmentation 
and in a page table, per segment, is then solved for a
probability density function of segment size which 
may be expressed as a convex combination of erlang densities.

cacm june, 1973

gelenbe, e.

dynamic storage allocation, virtual memory, paging,
multiple page sizes, 
************************

************************
Document:  CACM-1884.html
************************
************************
Document:  CACM-1025.html
************************
************************
Document:  CACM-2785.html
************************
************************
Document:  CACM-2069.html
************************
************************
Document:  CACM-3166.html
************************
************************
Document:  CACM-1957.html
************************


the list set generator: a construct for evaluating set expressions

the list set generator is defined and algorithms
for its use are given.  the list set generator is
a construct which may be added to a list processing system or any
system that handles sets.  it efficiently generates the set which
results from any expression involving sets and set operators.  the
efficiency derives from evaluating the expression as a whole and
in parallel, rather than evaluating 
************************

************************
Document:  CACM-3153.html
************************


the control of response times in multi-class
systems by memory allocations 

the possibility of giving different quality of service to jobs of different
classes by regulating their memory allocation is examined in
the context of a paged computer system.  two parameterized algorithms
which partition the main memory between two classes of jobs are
considered.  initially, a closed system consisting of a process
or and paging and file devices, with 
************************
 and file devices, with fixed numbers of jobs, is studied
to determine optimal degrees of multiprogramming and the proportion
of processor time devoted to each class.  applying a decomposition
approach and treating the closed system as a single server,
the response times in an open system with external arrivals are
studied.  the object is to investigate the effect of the memory
alocation parameters on the expected response times under the two algorithms.
numerical solutions and economical lower bounds for the
expected response times as functions of the control parameters
are obtained.  a way of applying the results to systems with more
than two job classes is indicated.

cacm july, 1979

hine, j.
mitrani, i.
tsur, s.

queueing networks, paging, virtual memory, performance 
************************

************************
Document:  CACM-1828.html
************************
************************
Document:  CACM-2740.html
************************


a large semaphore based operating system

the paper describes the internal structure of
a large operating system as a set of cooperating 
sequential processes.  the processes synchronize by
means of semaphores and extended semaphores (queue 
semaphores).  the number of parallel processes is carefully
justified, and the various semaphore constructions 
are explained.  the system is proved to be free of "deadly
embrace" (deadlock).  the design principle 
is an alternative to dijkstra's hierarchical structuring
of operating systems.  the project management 
and the performance are discussed, too.  the operating
system is the first large one using the rc 4000 
multiprogramming system.

cacm july, 1975

lauesen, s.

cooperating processes, operating system, semaphores,
semaphore applications, queue semaphores, 
deadlock, deadly embrace, hierarchical structuring, multiprogramming,
operating system structure, asynchronous 
structuring, buffering, parallel processes, synchronizing
primitives, reentrant code, rc 4000, project 
management, time schedule, debugging, project planning,
project scheduling, reliability, program proving, 
coroutines, correctness, program maintenance, software paging

4.30 4.31 4.32 4.42 
************************

************************
Document:  CACM-2289.html
************************


cellular arrays for the solution of graph problems

a cellular array is a two-dimensional, checkerboard
type interconnection of identical modules 
(or cells), where each cell contains a few bits of
memory and a small amount of combinational logic, 
and communicates mainly with its immediate neighbors
in the array.  the chief computational advantage 
offered by cellular arrays is the improvement in speed
achieved by virtue of the possibilities for parallel 
processing.  in this paper it is shown that cellular
arrays are inherently well suited for the solution 
of many graph problems.  for example, the adjacency
matrix of a graph is easily mapped onto an array; 
each matrix element is stored in one cell of the array,
and typical row and column operations are readily 
implemented by simple cell logic.  a major challenge
in the effective use of cellular arrays for the 
solution of graph problems is the determination of algorithms
that exploit the possibilities 
************************

that exploit the possibilities for parallelism, 
especially for problems whose solutions appear to be inherently
serial.  in particular, several parallelized 
algorithms are presented for the 
************************
 are presented for the solution of certain
spanning tree, distance, and path problems, with 
direct applications to wire routing, pert chart analysis,
and the analysis of many types of networks. 
 these algorithms exhibit a computation time that in
many cases grows at a rate not exceeding log2 n, 
where n is the number of nodes in the graph.  straightforward
cellular implementations of the well-known 
serial algorithms for these problems require about n
steps, and noncellular implementations require from 
n^2 to n^3 steps.

cacm september, 1972

levitt, k. n.
kautz, w. h.

graph theory, cellular logic-in-memory arrays,
parallel processing, special 
************************

************************
Document:  CACM-2016.html
************************
************************
Document:  CACM-2667.html
************************
************************
Document:  CACM-1752.html
************************


resource management for a medium scale time-sharing operating system

task scheduling and resource balancing for
a medium size virtual memory paging machine are 
discussed in relation to a combined batch processing
and time-sharing environment.  a synopsis is given 
of the task scheduling and paging algorithms that were implemented,

************************
 that were implemented,
and the results of comparative simulation 
are given by tracing the development of the algorithms
through six predecessor versions.  throughout 
the discussion particular emphasis is placed on balancing
the system performance relative to the characteristics 
of all the system resources.  simulation results relative
to alternate hardware characteristics and the 
effects of program mix and loading variations are also presented.

cacm may, 1968

oppenheimer, g.
weizer, n.

time-sharing, operating systems, resource management,
task scheduling, paging, system simulation, 
************************

************************
Document:  CACM-1684.html
************************
************************
Document:  CACM-2277.html
************************
************************
Document:  CACM-2128.html
************************


a processor allocation method for time-sharing

a scheduling algorithm is proposed which is intended to minimize changes of 
tasks on processors and thereby reduce over-head.  the algorithm also has
application to more general resource allocation problems.  it is implemented 
by means of a method for efficiently handling dynamically changing segmented 
lists.

cacm january, 1970

mullery, a. p.
driscoll, g. c.

time sharing, resource allocation, scheduling algorithms,
monitors, dynamic allocation, 
************************

************************
Document:  CACM-1158.html
************************
************************
Document:  CACM-1569.html
************************
************************
Document:  CACM-0141.html
************************
************************
Document:  CACM-2579.html
************************


register allocation via usage counts

this paper introduces the notion of usage counts,
shows how usage counts can be developed by 
algorithms that eliminate redundant computations, and
describes how usage counts can provide the basis 
for register allocation.  the paper compares register
allocation based on usage counts to other commonly 
used register allocation techniques, and presents evidence
which shows that the usage count technique 
is significantly better than these other techniques.

cacm november, 1974

freiburghouse, r. a.

optimization, redundant computations, common subexpressions,
register allocation, compilers, programming 
languages, virtual memory, demand paging

4.12 4.2 4.39

ca741105 
************************

************************
Document:  CACM-1728.html
************************


further experimental data on the behavior
of programs in a paging environment

results are summarized from an empirical study
directed at the measurement of program operating 
behavior in those multiprogramming systems in which
programs are organized into fixed length pages.  
the data collected from the interpretive execution of
a number of paged programs are used to describe 
the frequency of page faults, i.e. the frequency of those
instants at which an executing program requires 
a page of data or instructions not in main (core) memory.
 these data are used also for the evaluation 
of page replacement algorithms and for assessing the

************************

************************
Document:  CACM-2714.html
************************


merging with parallel processors

consider two linearly 
************************


consider two linearly ordered sets a, b, |a|=m,
|b|=n, m<=n, and p, p<=m, parallel processors 
working synchronously. 
************************
 
working synchronously.  the paper presents an algorithm
for merging a and b with the p parallel processors, 
which requires at 
************************

************************
Document:  CACM-1747.html
************************
************************
Document:  CACM-2819.html
************************
************************
Document:  CACM-1262.html
************************
************************
Document:  CACM-2401.html
************************
