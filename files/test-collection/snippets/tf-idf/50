Snippets for the query:  Q50
************************
Total Hits:  43
************************

************************
Document:  CACM-3151.html
************************
************************
Document:  CACM-2884.html
************************
************************
Document:  CACM-2692.html
************************


reentrant polygon clipping

a new family of clipping algorithms is described.
 these algorithms are able to clip polygons 
against irregular convex plane-faced volumes in three
dimensions, removing the parts of the polygon which 
lie outside the volume.  in two dimensions the algorithms
permit clipping against irregular convex windows. 
 polygons to be clipped are represented as an ordered
sequence of vertices without repetition of first 
and last, in marked contrast to representation as a
collection of edges as was heretofore the common 
procedure.  output polygons have an identical format,
with new vertices introduced in sequence to describe 
any newly-cut edge or edges.  the algorithms easily handle
the particularly difficult problem of detecting 
that a new vertex may be required at a corner of the
clipping window.  the algorithms described achieve 
considerable simplicity by clipping separately against
each clipping plane or window boundary.  code 
capable of clipping the polygon against a single boundary
is reentered to clip against subsequent boundaries. 
 each such reentrant stage of clipping need store only
two vertex values and may begin its processing 
as soon as the first output vertex from the proceeding
stage is ready.  because the same code is reentered 
for clipping against subsequent boundaries, clipping
against very complex window shapes is practical. 
 for perspective applications in three dimentions, a six-plane
truncated pyramid is chosen as the clipping 
volume.  the two additional planes parallel to the projection
screen 
************************

************************
Document:  CACM-3059.html
************************
************************
Document:  CACM-1953.html
************************
************************
Document:  CACM-1796.html
************************
************************
Document:  CACM-2997.html
************************
************************
Document:  CACM-1665.html
************************
************************
Document:  CACM-3152.html
************************


storage reorganization techniques for
matrix computation in a paging environment

in order to multiply matrices while minimizing
the number of page fetches required, it is often more efficient to
reorganize the data into submatrix form and to use block multiplication 
rather than to use the best known algorithms which leave the
matrices 
************************

************************
Document:  CACM-2068.html
************************
************************
Document:  CACM-2283.html
************************
************************
Document:  CACM-2146.html
************************
************************
Document:  CACM-0082.html
************************
************************
Document:  CACM-3156.html
************************


computing connected components on parallel computers

we present a parallel algorithm which uses n2 processors to find the connected

************************
 to find the connected
components of an undirected graph with n vertices in time o(log2n).  an
o(log2n) time bound also can be achieved using only n$n/$log2n)) processors.
the algorithm can be used to find the transitive closure
of a symmetric boolean matrix.  we assume that the processors have
access to a common memory.  simultaneous access to the same location
is permitted for fetch instructions but not for store instructions.

cacm august, 1979

hirschberg, d.
chandra, a.
sarwate, d.

graph theory, parallel processing, algorithms,
transitive closure, 
************************

************************
Document:  CACM-2266.html
************************


a highly parallel algorithm for approximating
all zeros of a polynomial with only real zeros

an algorithm is described based on newton's
method which simultaneously approximates all zeros 
of a polynomial with only real zeros.  the algorithm, which
is conceptually suitable for parallel computation, 
determines its own starting values so that convergence
to the zeros is guaranteed.  multiple zeros and 
their multiplicity are readily determined.  at no
point in the method is polynomial deflation used.

cacm november, 1972

patrick, m. l.

parallel numerical algorithms, real polynomials,
real 
************************

************************
Document:  CACM-2137.html
************************
************************
Document:  CACM-2401.html
************************
************************
Document:  CACM-1964.html
************************
************************
Document:  CACM-2342.html
************************
************************
Document:  CACM-2175.html
************************


subexpression ordering in the execution of arithmetic expressions

an arithmetic expression can often be broken
down into its component subexpressions.  depending 
on the hardware environment in which the expression is
to be executed, these subexpressions can be evaluated 
in serials, in parallel, or in a combination of these
modes.  this paper shows that expression execution 
time can be minimized only if consideration is given to
the ordering of the subexpressions.  in particular, 
subexpressions should be executed in order of decreasing
memory and processor time requirements.  this 
observation is valid for configurations ranging from
a uniprocessor with an unbuffered main memory to 
multiprocessor with a "cache" buffer memory.  if the
number of subexpressions which can be executed in 
parallel exceeds the number of available processors,
then execution of some 
************************

************************
Document:  CACM-1302.html
************************
************************
Document:  CACM-1658.html
************************
************************
Document:  CACM-0371.html
************************
************************
Document:  CACM-2627.html
************************
************************
Document:  CACM-2557.html
************************


on the time required for a sequence of matrix products

this paper discusses the multiplication of conformable
sequences of row vectors, column vectors, 
and square matrices.  the minimum time required to evaluate
such products on ordinary serial computers 
as well as parallel computers is discussed.  algorithms
are presented which 
************************

************************
Document:  CACM-2700.html
************************
************************
Document:  CACM-1924.html
************************


organizing matrices and matrix operations for paged memory systems

matrix representations and operations are examined
for the purpose of minimizing the page faulting 
occurring in a paged memory system.  it is shown that
carefully designed matrix algorithms can lead to 
enormous savings in the number of page faults occurring
when only a small part of the total matrix can 
be in main memory at one time.  examination of addition,
multiplication, and inversion algorithms shows 
that a partitioned matrix representation (i.e. one submatrix
or partition per page) in most cases induced 
fewer page faults than a row-by-row representation.
 the number of page-pulls required by these matrix 
manipulation algorithms is also studied as a function
of the number of pages of main memory available 
to the algorithm.

cacm march, 1969

mckellar, a. c.
coffman jr., e. g.

matrix algorithms, array processing, paging algorithms,
paged memory 
************************

************************
Document:  CACM-2667.html
************************
************************
Document:  CACM-3075.html
************************


fast parallel sorting algorithms

a parallel bucket-sort 
************************


a parallel bucket-sort algorithm is presented
that requires time o(log n) and the use of n 
processors.  the algorithm makes 
************************
.  the algorithm makes use of a technique that
requires more space than the product of processors 
and time.  a realistic model is used model is used in which
no memory contention is permitted.  a procedure 
is also presented to sort n numbers in time o(k log
n) using n 1 + 1/k processors, for k an arbitrary 
integer.  the model of computation for this procedure
permits simultaneous fetches from the same memory 
location.

cacm august, 1978

hirschberg, d.

parallel processing, sorting, algorithms, bucket sort

3.74 4.34 
************************

************************
Document:  CACM-3006.html
************************


anomalies with variable partition paging algorithms

five types of anomalous 
************************


five types of anomalous behavior which may
occur in paged virtual memory operating systems 
a redefined.  one type of anomaly, for example, concerns
the fact that, with certain reference strings 
and paging algorithms, an increase in mean 
************************
, an increase in mean memory allocation
may result in an increase in fault rate. 
 two paging algorithms, are examined in terms 
************************
, are examined in terms of their
anomaly potential, and reference string examples 
of various anomalies are presented.  two paging algorithm
properties, the inclusion property and the 
generalized inclusion property, are discussed and the
anomaly implications of these properties presented.

cacm march, 1978

franklin, m.
graham, g.
gupta, r.

anomaly, memory management, program behavior, stack
algorithms, virtual memory, working 
************************
, virtual memory, working set, page 
fault frequency, paging algorithms

4.32 4.35 4.6 8.1


************************

************************
Document:  CACM-2433.html
************************


control structures in illiac iv fortran

as part of an effort to design and implement
a fortran compiler on the illiac iv, an extended 
fortran, called ivtran, has been developed.  this language
provides a means of expressing data and control 
structures suitable for exploiting illiac iv parallelism.
 this paper reviews the hardware characteristics 
of the illiac and singles out unconventional features
which could be expected to influence language (and 
compiler) design.  the implications of these features for
data layout and algorithm structure are discussed, 
and the conclusion is drawn that data allocation rather than
code structuring is the crucial illiac optimization 
problem.  a satisfactory method of data allocation is
then presented.  language structures to utilize 
this storage method and express parallel algorithms are described.

cacm 
************************

************************
Document:  CACM-2226.html
************************
************************
Document:  CACM-1811.html
************************


a case study in programming for parallel-processors

an affirmative partial 
************************


an affirmative partial answer is provided to
the question of whether it is possible to program 
parallel-processor computing systems to efficiently decrease
execution time for useful problems.  parallel-processor 
systems are multiprocessor systems in which several of
the processors can simultaneously execute 
************************
 can simultaneously execute separate 
tasks of a single job, thus cooperating to decrease
the solution time of a computational problem. the 
processors have independent instruction counters, meaning
that each processor executes its own task program 
relatively independently of the other processors.  communication
between cooperating processors is by 
means of data in storage shared by all processors.  a
program for the determination of the distribution 
of current in an electrical network was written for a
parallel-processor computing 
************************
-processor computing system, and execution 
of this program was simulated.  the data gathered from
simulation runs demonstrate the efficient solution 
of this problem, typical of a large class of important
problems.  it is shown that, with proper programming, 
solution time when n processors are applied approaches
1/n times the solution time for a single processor, 
while improper programming can actually lead to an increase
of solution time with the number of processors. 
 stability of the method of solution was also investigated.

cacm december, 1969

rosenfeld, j. l.

parallel-processor, parallelism, 
************************

************************
Document:  CACM-2365.html
************************


matrix computations with fortran and paging

the efficiency of conventional fortran programs
for matrix computations can often be improved 
by reversing the order of nested loops.  such modifications
produce modest savings in many common situations 
and very significant savings for large problems run
under an operating system which uses paging.

cacm april, 1972

moler, c. b.

matrix algorithms, linear equations, fortran,

************************

************************
Document:  CACM-0950.html
************************


parallel methods for integrating ordinary differential equations

this paper is dedicated to the proposition that,
in order to take full advantage for real-time 
computations of highly parallel computers as can be
expected to be available in the near future, much 
of numerical analysis will have to be recast in a more
"parallel" form.  by this is meant that serial 
algorithms ought to be replaced 
************************

************************
Document:  CACM-2895.html
************************
************************
Document:  CACM-1660.html
************************
************************
Document:  CACM-2685.html
************************
************************
Document:  CACM-2085.html
************************
************************
Document:  CACM-1873.html
************************
************************
Document:  CACM-2914.html
************************
************************
Document:  CACM-1851.html
************************
************************
Document:  CACM-2669.html
************************
************************
Document:  CACM-0392.html
************************
************************
Document:  CACM-2973.html
************************


sorting on a mesh-connected parallel computer

two algorithms are presented for sorting 
************************
 are presented for sorting n^2
elements on an n x n mesh-connected processor 
array that require o(n) routing and comparison steps.
 the best previous algorithm takes time o(n log 
n).  the algorithms of this paper are shown to be optimal
in time within small constant factors.  extensions 
to higher-dimensional arrays are also given.

cacm april, 1977

thompson, c. d.
kung, h. t.

parallel computer, parallel sorting, 
************************

************************
Document:  CACM-2426.html
************************
************************
Document:  CACM-1952.html
************************
************************
Document:  CACM-2950.html
************************
************************
Document:  CACM-2267.html
************************
************************
Document:  CACM-1795.html
************************
************************
Document:  CACM-2903.html
************************
************************
Document:  CACM-2570.html
************************


a comparison of list schedules for parallel processing systems

the problem of scheduling two or more processors
to minimize the execution 
************************

to minimize the execution time of a program 
which consists of a set of partially ordered tasks
is studied.  cases where task execution times are 
deterministic and others in which execution times are
random variables are analyzed.  it is shown that 
different algorithms suggested in the literature vary significantly
in execution time and that the b-schedule 
of coffman and graham is near-optimal.  a dynamic programming
solution for the case in which execution 
times are random variables is presented.

cacm december, 1974

adam, t. l.
chandy, k. m.
dickson, j. r.

parallel processing, precedence 
************************

************************
Document:  CACM-2195.html
************************
************************
Document:  CACM-0804.html
************************
************************
Document:  CACM-2022.html
************************
************************
Document:  CACM-1601.html
************************


parallel numerical methods for the solution of equations

classical iterative procedures for the numerical
solution of equations provide at each stage 
a single new approximation to the root in question.  a
technique is given for the development of numerical 
procedures which provide, at each stage, several approximations
to a solution of an equation.  the s8everal 
approximations obtained in any iteration are computationally
independent, making the methods of interest 
in a parallel processing environment.  convergence is
insured by extracting the "best information" at 
each iteration.  several families of numerical procedures
which use the technique of the procedures in 
a parallel processing environment are developed and measurements
of these statistics are reported.  these 
measurements are interpreted in a parallel processing
environment.  in such an environment the procedures 
obtained are superior to standard algorithms.

cacm may, 1967

shedler, 
************************

************************
Document:  CACM-0270.html
************************
************************
Document:  CACM-2297.html
************************
************************
Document:  CACM-2007.html
************************
************************
Document:  CACM-1810.html
************************


is automatic "folding" of programs efficient enough to displace manual?

the operation of "folding" a program into
the available memory is discussed.  measurements 
by brown et al. and by nelson on an automatic folding
mechanism of simple design, a demand paging unit 
built at the ibm research center by belady, nelson,
o'neil, and others, permitting its quality to be 
compared with that of manual folding, are discussed,
and it is shown that given some care in use the 
unit performs satisfactorily under the conditions tested,
even though it is operating across a memory-to-storage 
interface with a very large speed difference.  the disadvantages
of prefolding, which is required when 
the folding is manual, are examined, and a number of
the important troubles which beset computing today 
are shown to arise from, or be aggravated by, this
source.  it is concluded that a folding mechanism 
will probably become a normal part of most computing systems.

cacm december, 1969

sayre, d.

paging, automatic paging, demand paging, folding,
automatic folding, storage hierarchies, memory 
hierarchies, replacement algorithms, performance, measurement

************************

************************
Document:  CACM-3132.html
************************
************************
Document:  CACM-1988.html
************************
************************
Document:  CACM-2450.html
************************


empirical working set behavior

the working set model for program behavior
has been proposed in recent years as a basis for 
the design of scheduling and paging algorithms.  although
the words 
************************

************************
Document:  CACM-2851.html
************************
************************
Document:  CACM-2904.html
************************
************************
Document:  CACM-2114.html
************************


a formal system for information retrieval from files

a generalized file structure is provided
by which the concepts of keyword, index, record, file, directory,
file structure, directory decoding, and record retrieval are defined
and from which some of the frequently used file structures such
as inverted files, index-sequential files, and multilist files are
derived.  two algorithms which retrieve records from the generalized file 
structure are presented.

cacm february, 1970

hsiao, d.

attribute-value pair, index, keyword, record, record address,
k-pointer, k-list, file, directory, generalized file
structure, inverted file, index-sequential-file, multilist file,
description, file search, directory search, serial processing of
lists, prime keyword, parallel processing of lists 
************************

************************
Document:  CACM-2953.html
************************
************************
Document:  CACM-1171.html
************************
************************
Document:  CACM-2490.html
************************
************************
Document:  CACM-1529.html
************************
************************
Document:  CACM-2714.html
************************


merging with parallel processors

consider two linearly 
************************


consider two linearly ordered sets a, b, |a|=m,
|b|=n, m<=n, and p, p<=m, parallel processors 
working synchronously. 
************************
 
working synchronously.  the paper presents an algorithm
for merging a and b with the p parallel processors, 
which requires at 
************************

************************
Document:  CACM-1342.html
************************
************************
Document:  CACM-1923.html
************************
************************
Document:  CACM-1367.html
************************
************************
Document:  CACM-1884.html
************************
************************
Document:  CACM-2785.html
************************
************************
Document:  CACM-1551.html
************************
************************
Document:  CACM-2069.html
************************
************************
Document:  CACM-3166.html
************************
************************
Document:  CACM-3153.html
************************


the control of response times in multi-class
systems by memory allocations 

the possibility of giving different quality of service to jobs of different
classes by regulating their memory allocation is examined in
the context of a paged computer system.  two parameterized algorithms
which partition the main memory between two classes of jobs are
considered.  initially, a closed system consisting of a process
or and paging and file devices, with 
************************
 and file devices, with fixed numbers of jobs, is studied
to determine optimal degrees of multiprogramming and the proportion
of processor time devoted to each class.  applying a decomposition
approach and treating the closed system as a single server,
the response times in an open system with external arrivals are
studied.  the object is to investigate the effect of the memory
alocation parameters on the expected response times under the two algorithms.
numerical solutions and economical lower bounds for the
expected response times as functions of the control parameters
are obtained.  a way of applying the results to systems with more
than two job classes is indicated.

cacm july, 1979

hine, j.
mitrani, i.
tsur, s.

queueing networks, paging, virtual memory, performance 
************************

************************
Document:  CACM-1828.html
************************
************************
Document:  CACM-2272.html
************************
************************
Document:  CACM-2725.html
************************
************************
Document:  CACM-2289.html
************************


cellular arrays for the solution of graph problems

a cellular array is a two-dimensional, checkerboard
type interconnection of identical modules 
(or cells), where each cell contains a few bits of
memory and a small amount of combinational logic, 
and communicates mainly with its immediate neighbors
in the array.  the chief computational advantage 
offered by cellular arrays is the improvement in speed
achieved by virtue of the possibilities for parallel 
processing.  in this paper it is shown that cellular
arrays are inherently well suited for the solution 
of many graph problems.  for example, the adjacency
matrix of a graph is easily mapped onto an array; 
each matrix element is stored in one cell of the array,
and typical row and column operations are readily 
implemented by simple cell logic.  a major challenge
in the effective use of cellular arrays for the 
solution of graph problems is the determination of algorithms
that exploit the possibilities 
************************

that exploit the possibilities for parallelism, 
especially for problems whose solutions appear to be inherently
serial.  in particular, several parallelized 
algorithms are presented for the 
************************
 are presented for the solution of certain
spanning tree, distance, and path problems, with 
direct applications to wire routing, pert chart analysis,
and the analysis of many types of networks. 
 these algorithms exhibit a computation time that in
many cases grows at a rate not exceeding log2 n, 
where n is the number of nodes in the graph.  straightforward
cellular implementations of the well-known 
serial algorithms for these problems require about n
steps, and noncellular implementations require from 
n^2 to n^3 steps.

cacm september, 1972

levitt, k. n.
kautz, w. h.

graph theory, cellular logic-in-memory arrays,
parallel processing, special 
************************

************************
Document:  CACM-1374.html
************************
************************
Document:  CACM-1752.html
************************


resource management for a medium scale time-sharing operating system

task scheduling and resource balancing for
a medium size virtual memory paging machine are 
discussed in relation to a combined batch processing
and time-sharing environment.  a synopsis is given 
of the task scheduling and paging algorithms that were implemented,

************************
 that were implemented,
and the results of comparative simulation 
are given by tracing the development of the algorithms
through six predecessor versions.  throughout 
the discussion particular emphasis is placed on balancing
the system performance relative to the characteristics 
of all the system resources.  simulation results relative
to alternate hardware characteristics and the 
effects of program mix and loading variations are also presented.

cacm may, 1968

oppenheimer, g.
weizer, n.

time-sharing, operating systems, resource management,
task scheduling, paging, system simulation, 
************************

************************
Document:  CACM-2767.html
************************
************************
Document:  CACM-2872.html
************************
************************
Document:  CACM-2674.html
************************
************************
Document:  CACM-2830.html
************************
************************
Document:  CACM-2277.html
************************
************************
Document:  CACM-2128.html
************************


a processor allocation method for time-sharing

a scheduling algorithm is proposed which is intended to minimize changes of 
tasks on processors and thereby reduce over-head.  the algorithm also has
application to more general resource allocation problems.  it is implemented 
by means of a method for efficiently handling dynamically changing segmented 
lists.

cacm january, 1970

mullery, a. p.
driscoll, g. c.

time sharing, resource allocation, scheduling algorithms,
monitors, dynamic allocation, 
************************

************************
Document:  CACM-1158.html
************************
************************
Document:  CACM-2942.html
************************
************************
Document:  CACM-1569.html
************************
************************
Document:  CACM-0141.html
************************
************************
Document:  CACM-2497.html
************************


synchronizing processors with memory-content-generated interrupts

implementations of the "lock-unlock" method
of synchronizing processors in a multiprocessor 
system usually require uninterruptable, memory-pause type instructions.
 an interlock scheme called read-interlock, 
which does not require memory-pause instructions, has
been developed for a dual dec pdp-10 system with 
real-time requirements.  the read-interlock method does
require a special"read-interlock" instruction 
in the repertoire of the processors and a special "read-interlock"
cycle in the repertoire of the memory 
modules.  when a processor examines a "lock" (a memory
location) with a read-interlock instruction, it 
will be interrupted if the lock was already set; examining
a lock immediately sets it if it was not already 
set (this event sequence is a read-interlock cycle). 
writing into a lock clears it.  having the processor 
interrupted upon encountering a set lock instead of
branching is advantageous if the branch would have 
resulted in an effective interrupt.

cacm june, 1973

hill, j. c.

interrupts,supervisors, monitors, debugging, parallel
processing, associative 
************************

************************
Document:  CACM-2819.html
************************
************************
Document:  CACM-1262.html
************************
************************
Document:  CACM-2896.html
************************
